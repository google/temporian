{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74f7111-1b6c-4454-9770-3f67eeadaca6",
   "metadata": {},
   "source": [
    "# Recipes: time-based aggregation\n",
    "\n",
    "The following use cases are covered here:\n",
    " - Use case 1: Sum daily sales, from individual item transactions.\n",
    " - Use case 2: Sum daily sales by store, from store/product sales.\n",
    " - Use case 3: Unify sensor measurements with the same timestamp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de274e0e-ab5a-46a0-b4b9-f1026b43076c",
   "metadata": {},
   "source": [
    "## Use case 1: Aggregate daily sales\n",
    "\n",
    "We have the sales log from a store, each item sale is represented by an event. They have a date-time, the price at which the product was sold, and the unit cost of the product.\n",
    "\n",
    "We want to calculate total daily sales. So this is what we do:\n",
    "- Create a uniform sampling with one tick per day (could be any other interval), at time `00:00:00`.\n",
    "- Add up all sales that happened between `00:00:01` from the previous day, and the current tick at `00:00:00`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a56d43-d011-4e72-aed4-8d460d58c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import temporian as tp\n",
    "\n",
    "sales_data = pd.DataFrame(\n",
    "    data=[\n",
    "        [\"2020-01-02 13:04\", 3.0,  1.0],\n",
    "        [\"2020-01-02 13:04\", 5.0,  2.0],  # duplicated timestamp\n",
    "        [\"2020-01-02 15:24\", 7.0,  3.0],\n",
    "        [\"2020-01-03 13:45\", 3.0,  1.0],\n",
    "        [\"2020-01-03 16:10\", 7.0,  3.0],\n",
    "        [\"2020-01-03 17:30\", 10.0, 5.0],\n",
    "    ],\n",
    "    columns=[\n",
    "        \"timestamp\",\n",
    "        \"unit_price\",\n",
    "        \"unit_cost\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "sales_evset = tp.from_pandas(sales_data)\n",
    "sales_evset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e930b36-10f6-487f-8466-278a1a08956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We want sales between these days\n",
    "time_span = tp.event_set(timestamps=[\"2020-01-01 00:00\", \"2020-01-07 00:00\"])\n",
    "\n",
    "# Create ticks at 00:00\n",
    "interval = tp.duration.days(1)\n",
    "ticks = time_span.tick(interval)\n",
    "\n",
    "ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92dff9-b4b6-42d1-8848-b37ff768ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we provide the ticks as sampling\n",
    "moving_sum = sales_evset.moving_sum(window_length=interval, sampling=ticks)\n",
    "\n",
    "moving_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664550bb-12ac-43c2-8216-4308843178b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally, rename and calculate profit\n",
    "daily_sales = moving_sum.rename({\"unit_price\": \"daily_revenue\", \"unit_cost\": \"daily_cost\"})\n",
    "\n",
    "daily_profit = (daily_sales[\"daily_revenue\"] - daily_sales[\"daily_cost\"]).rename(\"daily_profit\")\n",
    "\n",
    "daily_sales = tp.glue(daily_sales, daily_profit)\n",
    "\n",
    "daily_sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d0511-8898-4135-b2bc-69f0ad5a2bd9",
   "metadata": {},
   "source": [
    "## Use case 2: Aggregate sales by store\n",
    "\n",
    "In this case, let's assume that we've multiple stores with multiple products being sold.\n",
    "\n",
    "We want to aggregate all product sales per store, so this is what we need to do:\n",
    "- Ignore the `product_id` index.\n",
    "- Unify daily sales per store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b25e47-8b27-4dab-baa3-c9b75745372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import temporian as tp\n",
    "\n",
    "\n",
    "sales_data = pd.DataFrame(\n",
    "    data=[\n",
    "        # store 1\n",
    "        [\"2020-01-01\", \"store_1\", \"product_1\", 300.0],\n",
    "        [\"2020-01-02\", \"store_1\", \"product_1\", 450.0],\n",
    "        [\"2020-01-03\", \"store_1\", \"product_1\", 600.0],\n",
    "        [\"2020-01-01\", \"store_1\", \"product_2\", 100.0],\n",
    "        [\"2020-01-02\", \"store_1\", \"product_2\", 250.0],\n",
    "        [\"2020-01-03\", \"store_1\", \"product_2\", 100.0],\n",
    "        # store 2\n",
    "        [\"2020-01-01\", \"store_2\", \"product_1\", 900.0],\n",
    "        [\"2020-01-02\", \"store_2\", \"product_1\", 750.0],\n",
    "        [\"2020-01-03\", \"store_2\", \"product_1\", 750.0],\n",
    "        [\"2020-01-01\", \"store_2\", \"product_3\", 20.0],\n",
    "        [\"2020-01-02\", \"store_2\", \"product_3\", 40.0],\n",
    "        [\"2020-01-03\", \"store_2\", \"product_3\", 30.0],\n",
    "    ],\n",
    "    columns=[\n",
    "        \"timestamp\",\n",
    "        \"store_id\",\n",
    "        \"product_id\",\n",
    "        \"sales_usd\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load data indexed by store/product\n",
    "sales_evset = tp.from_pandas(sales_data, indexes=[\"store_id\", \"product_id\"])\n",
    "sales_evset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d15bf-a58e-4ad0-a44c-272ff449150c",
   "metadata": {},
   "source": [
    "Note that for each store/product we've the same 3 days of sales.\n",
    "\n",
    "Now we want to calculate sales for each store, regardless of the `product_id`.\n",
    "\n",
    "So, the first step is to remove that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54eb39-7e38-479b-b926-40b12dd48a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales = sales_evset.drop_index(\"product_id\")\n",
    "\n",
    "store_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810d461-631d-4453-9ae1-0352024427a7",
   "metadata": {},
   "source": [
    "As you can see, now we've each timestamp duplicated, one for each product.\n",
    "\n",
    "We want to discard the `product_id` information, and unify the timestamps adding up the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28343c-d3f1-48df-8f1e-0fd4ee58505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_days = store_sales.unique_timestamps()\n",
    "\n",
    "store_daily_sales = store_sales[\"sales_usd\"].moving_sum(window_length=tp.duration.days(1), sampling=unique_days)\n",
    "\n",
    "store_daily_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a41a5-bd95-4588-bad3-83691bd0acd0",
   "metadata": {},
   "source": [
    "## Use case 3: Unify sensor measurements\n",
    "\n",
    "In this case we'll show how to avoid having events with duplicated timestamps.\n",
    "\n",
    "It's different to the previous cases because:\n",
    " - The original timestamps must be preserved (i.e: can't use `tick()`).\n",
    " - The moving window can't cover two different timestamps (i.e: shortest `window_length` available)\n",
    "\n",
    "Let's define some non-uniform timestamps to illustrate the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e57e74-ca80-4292-834a-e7cfd9185b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import temporian as tp\n",
    "\n",
    "sensor_evset = tp.event_set(timestamps=[1.1, 2.01, 2.02, 2.02, 3.5, 3.51, 3.51, 4.5, 5.0],\n",
    "                            features={\"y\": [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
    "                                      \"z\": [10., 20., 30., 40., 50., 60., 70., 80., 90.]\n",
    "                                     }\n",
    "                           )\n",
    "sensor_evset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b875a15-cd14-49bd-a83e-301f9c7aef17",
   "metadata": {},
   "source": [
    "The first step is to create a new sampling removing the duplicated timestamps at `2.02` and `3.51`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b1aa0-c808-4905-81bf-e182b221468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated timestamps\n",
    "unique_t = sensor_evset.unique_timestamps()\n",
    "unique_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5a3e8-2975-4cda-a479-ba9efa219339",
   "metadata": {},
   "source": [
    "Next, we'll need a moving window but making sure that the window length doesn't overlap with two different timestamps at any point. It must be smaller than the smallest possible step.\n",
    "\n",
    "In `tp.duration.shortest`, we've defined the shortest possible interval that can be represented with a `float64` timestamp at maximum resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fbeba-5efc-437e-baac-9b38a536fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_length = tp.duration.shortest\n",
    "shortest_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d58e94-bd23-4d9e-bb53-88c299af65f5",
   "metadata": {},
   "source": [
    "Pretty small, right? Since null durations are not allowed, this is as close to zero as we can get. It's guaranteed that you'll never overlap two different timestamps using this.\n",
    "\n",
    "Now we just need to run the aggregation function that we need, providing the `shortest_length` and the unique timestamps as sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f1562-8c95-4b22-a40f-90b7ee751e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_evset = sensor_evset.simple_moving_average(window_length=shortest_length, sampling=unique_t)\n",
    "unified_evset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0d5a5-e2e7-406c-84c6-dd8ac717128d",
   "metadata": {},
   "source": [
    "Of course, instead of the average value, other moving window operations like `moving_min` or `moving_max` could make more sense depending on the use case, this is just an illustrative example.\n",
    "\n",
    "Also, keep in mind that this exact procedure would work well in an `EventSet` with multiple indexes, removing the duplicated timestamps in each index separately.\n",
    "\n",
    "But let's keep the example simple for now ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9c6e6-5cba-4950-900f-1878e87a98be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
